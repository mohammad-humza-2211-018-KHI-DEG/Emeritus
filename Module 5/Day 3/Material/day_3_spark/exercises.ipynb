{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b7a6ab7",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "In this notebook, we go through a simple example of how to use Spark Streaming with Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "754cfeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "import pyspark\n",
    "from IPython.display import clear_output, display\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.streaming import StreamingQuery\n",
    "from pyspark.sql.types import DateType, IntegerType, StringType, StructType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791650ea",
   "metadata": {},
   "source": [
    "1. We create a SparkSession in a local setup. We add the required libraries and check if we can access the Spark Context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17d7c1b9-f81f-41d1-a7d4-3522a23e9906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[1] appName=Tutorial App>\n"
     ]
    }
   ],
   "source": [
    "# Create Spark Session\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[1]\")\n",
    "    .appName(\"Tutorial App\")\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Get Spark Context\n",
    "print(spark.sparkContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53044541-d24c-4b61-b2bb-12ed4d6e8795",
   "metadata": {},
   "source": [
    "2. We can add the following messages in the console window with the Kafka producer. Later in Spark, we will be able to parse the data and apply simple transformations.\n",
    "\n",
    "```bash\n",
    "{\"id\":1,\"name\":\"Simon\",\"birth_date\":\"1998-07-01\"}\n",
    "{\"id\":2,\"name\":\"Anne\"}\n",
    "{\"id\":3,\"name\":\"John\",\"birth_date\":\"2000-04-23\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470903ec",
   "metadata": {},
   "source": [
    "3. We have to read our data stream. To do that we will use the `readStream` function from the Spark session.\n",
    "    * `format` will define our input source type. There are a few built-in sources:\n",
    "        * Kafka source - we will be using Kafka to read our data stream,\n",
    "        * File source - reads data from the given directory (files processed in order based on modification time)\n",
    "        * Socket source\n",
    "        * Rate source\n",
    "        * Rate Micro-Batch source\n",
    "        \n",
    "        For more information on input sources, see: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#input-sources\n",
    "        \n",
    "    * Now we have to define options for reading data from Kafka.\n",
    "        * We are running our broker on localhost with port 9092\n",
    "        * We want to subscribe to the topic `events` that we created earlier\n",
    "        * We define `startingOffsets` - `earliest` reads from starting offset while `latest` reads newly discovered events\n",
    "        \n",
    "        For more information on Spark Streaming and Kafka integration see: https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a51f2c8-e7e4-41f4-bce3-38eb88ef47a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data stream\n",
    "df_stream = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"subscribe\", \"events\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221861ad-181b-43ec-876d-cef6df4e11b0",
   "metadata": {},
   "source": [
    "4. After loading data we can print its schema. Which columns should we consider metadata and which contain our actual data?\n",
    "    * `value` will be our binary data other columns can be confidered matdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0dc3dc4-1be0-4dc9-93ba-654d4ef7557a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing schema\n",
    "df_stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c775dbb-a655-4efb-ad70-c6c863546e8c",
   "metadata": {},
   "source": [
    "5. We can extract information from binary data by defining data schema. In this example, we are reading JSON messages that are cast to string and parsed with the `from_json` function. Our custom schema is based on the data samples that we sent to a topic. We are also appending `timestamp` to a final DataFrame. This way we are able to see the time when the event was processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0db8db2d-976f-498c-a304-78634ba5fd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining new custom data schema\n",
    "schema = (\n",
    "    StructType()\n",
    "    .add(\"id\", IntegerType())\n",
    "    .add(\"name\", StringType())\n",
    "    .add(\"birth_date\", DateType())\n",
    ")\n",
    "\n",
    "# Parsing binary data + timestamp,\n",
    "df_stream = df_stream.selectExpr(\"CAST(value AS STRING)\", \"timestamp\").select(\n",
    "    from_json(col(\"value\"), schema).alias(\"sample\"), \"timestamp\"\n",
    ")\n",
    "\n",
    "# Selecting samples\n",
    "df_stream = df_stream.select(\"sample.*\", \"timestamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11cc64f-2518-4f89-ba29-9e2ccfe0f0bf",
   "metadata": {},
   "source": [
    "6. We can print the schema again to show the changes in the DataFrame structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a81797e-e233-4dcd-8206-811a1a43fb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing schema\n",
    "df_stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09d82aa-5ffd-4898-b7bb-7a38944ad785",
   "metadata": {},
   "source": [
    "7. Now we can run some simple transformations or filters. Remember that transformations defined in this step must be compatible with the output mode that you will use later. For more information see https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "636ac120-ef1a-44f0-8759-7f5836301366",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stream = df_stream.where(col(\"name\") != \"John\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dcb7e9-98df-4991-8f3a-cdb182e726d7",
   "metadata": {},
   "source": [
    "8. To see the transformations result we have to use the `writeStream` method. We can use various output sinks such as file or Kafka. For debugging purposes, we will use a memory sink. For more information about output sinks, see https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks. It is also essential to define a proper output mode:\n",
    "    * `append` mode (default) - used when rows added to the table are not going to change. Only new rows will be added to the result table. This will ensure that every row will be processed only once;\n",
    "    * `complete` mode - the whole result table will be outputted to the sink after every trigger;\n",
    "    * `update` model - only rows that were updated since the last trigger will be outputted to a sink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da322c3d-bf47-4272-88d0-f3f04d353ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating write stream\n",
    "query = (\n",
    "    df_stream.writeStream.outputMode(\"append\")\n",
    "    .format(\"memory\")\n",
    "    .queryName(\"query_name\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba613650-5039-44d6-89bb-db5ed5566cb4",
   "metadata": {},
   "source": [
    "`Trigger` is an additional parameter that we didn't use in the `writeStream` definition but is important in stream processing. This parameter defines query execution timing. Queries can be executed in a micro-batch manner or as a continuous process (experimental feature).\n",
    "\n",
    "For more info about triggers see: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#triggers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d259c9-224d-458e-bdec-7b61dc1e56d1",
   "metadata": {},
   "source": [
    "9. Right now our query is running in the background. We are dealing with a data stream so we can't simply treat it as a batch of data where computations end after some time. To check the status of our computations in a Jupyter notebook, we can use this handy `check_query` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4df327c7-d13b-4d5b-9f93-f3b49f02f8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_query(query: StreamingQuery, sleep_time: int = 5) -> None:\n",
    "    while True:\n",
    "        clear_output(wait=True)\n",
    "        display(query.status)\n",
    "        display(spark.sql(f\"SELECT * FROM {query.name}\").show(100))\n",
    "        sleep(sleep_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf04ff52-0aa4-43bb-90ad-f583de78777f",
   "metadata": {},
   "source": [
    "10. Now you can try to add your own examples with Kafka producer.\n",
    "\n",
    "**Remember to stop the execution of the last cell before moving to the next exercises (the `check_query` function is running in an infinite loop).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50d61de4-af85-43fb-977d-7cb3e8ba1a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Waiting for data to arrive',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+--------------------+\n",
      "| id| name|birth_date|           timestamp|\n",
      "+---+-----+----------+--------------------+\n",
      "|  1|Simon|2019-07-01|2022-12-28 10:11:...|\n",
      "|  2| Anne|2015-12-15|2022-12-28 10:11:...|\n",
      "|  1|Simon|2019-07-01|2022-12-28 10:11:...|\n",
      "|  1|Simon|2019-07-01|2022-12-28 10:23:...|\n",
      "|  1|Simon|2019-07-01|2022-12-28 10:35:...|\n",
      "|  1|Simon|2019-07-01|2022-12-28 16:13:...|\n",
      "|  1|Simon|2019-07-01|2022-12-28 16:13:...|\n",
      "|  1|Simon|1998-07-01|2022-12-29 07:07:...|\n",
      "|  2| Anne|      null|2022-12-29 07:07:...|\n",
      "|  1|Johny|      null|2022-12-29 07:43:...|\n",
      "+---+-----+----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Checking results in an infinite loop\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcheck_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m, in \u001b[0;36mcheck_query\u001b[0;34m(query, sleep_time)\u001b[0m\n\u001b[1;32m      4\u001b[0m display(query\u001b[38;5;241m.\u001b[39mstatus)\n\u001b[1;32m      5\u001b[0m display(spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m100\u001b[39m))\n\u001b[0;32m----> 6\u001b[0m \u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msleep_time\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Checking results in an infinite loop\n",
    "check_query(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "ead1b95f633dc9c51826328e1846203f51a198c6fb5f2884a80417ba131d4e82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
