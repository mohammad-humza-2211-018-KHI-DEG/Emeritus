{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3. Introduction to pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPython Notebook option to show plots in the notebook (not in a separate window)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.random.random([10, 5])\n",
    "df = pd.DataFrame(arr, columns=[\"col_0\", \"col_1\", \"col_2\", \"col_3\", \"col_4\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try some more sophisticated data selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows from a pd.DataFrame, for which 'col_0' has value >0.5\n",
    "df[df[\"col_0\"] > 0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nicer way for the same operation\n",
    "df.query(\"col_0 > 0.3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also access local variables and create more complicated expressions\n",
    "threshold = 0.5\n",
    "df.query(\"col_0 > @threshold or col_1 < @threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding new column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how to create new columns of our dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"constant_column\"] = 1\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"copied_value\"] = df[\"col_0\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"copied_and_doubled\"] = 2 * df[\"col_0\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works similarly for over operations like adding, subtracting, dividing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"combination_of_columns\"] = df[\"col_0\"] + 3 * df[\"col_3\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we need to create a more complicated computation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_function(entire_row):\n",
    "    if entire_row[\"col_2\"] > entire_row[\"col_4\"]:\n",
    "        return \"col_2 is larger than col_4\"\n",
    "    else:\n",
    "        return \"col_2 is not larger than col_4\"\n",
    "\n",
    "\n",
    "df[\"custom_column\"] = df.apply(custom_function, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas gives us also a nice opportunity to check simple statistical properties at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"new_col\"] = np.random.randint(0, 3, df.shape[0])  # Let's create new column\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series has this cool method to inspect number of occurrences of each value\n",
    "df[\"new_col\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we can list all unique values in a Series\n",
    "df[\"new_col\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the column\n",
    "df.drop(\"new_col\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"col_1\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"col_1\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"col_2\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"custom_column\")[\"col_2\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"custom_column\")[\"col_2\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also create new columns with results of group aggregations\n",
    "df[\"col_2_mean_in_group\"] = df.groupby(\"custom_column\")[\"col_2\"].transform(\"mean\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And let's have a look at some other basic data manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"new_col\"] = df[\"col_1\"] / df[\"col_0\"].max()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To drop columns we have to specify axis=1, default axis=0\n",
    "df.drop(\"new_col\", inplace=True, axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a new column\n",
    "df[\"new_col\"] = np.random.randint(0, 3, df.shape[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And map its values to some new ones\n",
    "my_map = {0: \"a\", 1: \"b\", 2: \"c\"}\n",
    "df[\"new_col\"] = df[\"new_col\"].map(my_map)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating throught DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also iterate over rows or subframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 100\n",
    "for i, row in df.iterrows():\n",
    "    df.loc[i, \"col_0\"] = row.loc[\"col_0\"] * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use itertuples which is faster than iterrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 100\n",
    "for row in df.itertuples():\n",
    "    df.loc[row.Index, \"col_0\"] = row.col_0 * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommended approach, the most efficient method to apply function along an axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 100\n",
    "df.loc[:, \"col_0\"] = df.apply(lambda row: row[\"col_0\"] * 2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, sub_df in df.groupby(\"new_col\"):\n",
    "    print(\"Label: {}\".format(label))\n",
    "    print(\"Subframe:\\n{}\\n\".format(sub_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can combine DataFrame in multiple ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.DataFrame(\n",
    "    {\n",
    "        \"A\": [\"A0\", \"A1\", \"A2\"],\n",
    "        \"B\": [\"B0\", \"B1\", \"B2\"],\n",
    "        \"C\": [\"C0\", \"C1\", \"C2\"],\n",
    "    },\n",
    "    index=[0, 1, 2],\n",
    ")\n",
    "\n",
    "df_2 = pd.DataFrame(\n",
    "    {\n",
    "        \"A\": [\"A4\", \"A5\", \"A6\"],\n",
    "        \"B\": [\"B4\", \"B5\", \"B6\"],\n",
    "        \"C\": [\"C4\", \"C5\", \"C6\"],\n",
    "    },\n",
    "    index=[3, 4, 5],\n",
    ")\n",
    "\n",
    "df_3 = pd.DataFrame(\n",
    "    {\n",
    "        \"A\": [\"A8\", \"A9\", \"A10\"],\n",
    "        \"B\": [\"B8\", \"B9\", \"B10\"],\n",
    "        \"C\": [\"C8\", \"C9\", \"C10\"],\n",
    "    },\n",
    "    index=[6, 7, 8],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat combines multiple DataFrames\n",
    "df_concat = pd.concat([df_1, df_2, df_3])\n",
    "print(df_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can associate specific keys with each of the DataFrames\n",
    "df_concat = pd.concat([df_1, df_2, df_3], keys=[\"df_1\", \"df_2\", \"df_3\"])\n",
    "print(df_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also join DataFrames in similar way to SQL tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.loc[:, \"key\"] = [\"K0\", \"K1\", \"K2\"]\n",
    "df_2.loc[:, \"key\"] = [\"K1\", \"K1\", \"K3\"]\n",
    "print(f\"df_1 =\\n{df_1}\\ndf_2 = \\n{df_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(df_1, df_2, on=\"key\", how=\"inner\")\n",
    "print(df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can choose which DataFrame we want to use the value from\n",
    "df_merged = pd.merge(df_1, df_2, on=\"key\", how=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values were filled with NaNs\n",
    "print(df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use both set of values\n",
    "df_merged = pd.merge(df_1, df_2, on=\"key\", how=\"outer\")\n",
    "print(df_merged)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
